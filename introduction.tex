The advancement of Large Language Models (LLMs) has significantly accelerated the progress of Artificial Intelligence (AI), extending the reach of Natural Language Processing (NLP) across numerous domains. These include language understanding \cite{brown2020language}, programming and code generation \cite{chen2021evaluating}, recommendation systems \cite{sun2023recommendation}, information retrieval \cite{ma2023instructretrieval}, mobile-device interaction and voice assistants \cite{rao2023speak}, scientific discovery \cite{nori2023capabilities}, medical question answering \cite{singhal2022large}, and legal reasoning \cite{katz2023gpt}. The release of powerful commercial LLMs such as ChatGPT \cite{openai2023gpt4}, Bard \cite{thoppilan2022lamda}, and Claude \cite{anthropic2023claude}, along with the open-source LLaMA models \cite{touvron2023llama, touvron2023llama2}, has spurred rapid growth in both academic research and real-world applications. However, the sheer scale of these models—often involving billions of parameters—introduces computational and financial burdens, limiting their accessibility in resource-constrained settings. Specifically, while these LLMs are powerful, their size and generality make it challenging to fully harness their capabilities, and this gap in utilization can hinder their effectiveness in specialized applications.

Knowledge Distillation (KD) \cite{hinton} is a widely used strategy for transferring the capabilities of LLMs into smaller, more deployable students. Mathematically, KD aims to minimize the divergence (usually Kullback–Leibler divergence) between the soft output distributions of a large teacher model and a smaller student model. Distilling into a 7B–13B model can reduce inference cost by 10×, making high-quality language generation feasible on resource-constrained hardware. However, standard KD methods are both computationally intensive and vulnerable to generalization failure. Two key challenges underlie these limitations: First, distribution mismatch arises when the student fails to match the teacher’s predictive distribution over all training examples. Since KD typically applies uniform supervision across the dataset, it can waste teacher compute on examples that are already easy for the student—offering little training signal while duplicating the teacher's token budget. Second, domain gap between the teacher’s pretraining distribution and the downstream deployment domain can cause up to 5–10 percentage point drops in performance unless both models undergo heavy continual pretraining \cite{adaptanddistill}. Such adaptation significantly increases cost, negating the benefits of distillation.

While entropy-aware distillation has shown that difficult examples contribute more to learning \cite{eakd}, no current method simultaneously (i) avoids full teacher passes, (ii) handles modest domain gaps with lightweight adaptation, and (iii) retains near-teacher performance. This proposal introduces DA-ESKD, a distillation framework that directly addresses both distribution mismatch and domain gap by using student-estimated uncertainty to selectively query the teacher and expanding supervision over time. The result is an efficient, adaptive KD pipeline that minimizes teacher compute without sacrificing accuracy.
