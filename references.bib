@article{bitdistiller,
  title={BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation},
  author={Zhang, Siyuan and Sun, Xuezhe and others},
  journal={arXiv preprint arXiv:2402.01438},
  year={2024}
}

@article{ohwefreeze,
  title={Oh! We Freeze: Layer Freezing for Stable Low-Bit Quantization-Aware Training of LLMs},
  author={Li, Xue and Li, Lei and others},
  journal={arXiv preprint arXiv:2312.00706},
  year={2023}
}

@article{llmqat,
  title={LLM-QAT: A Unified and Effective Framework for Quantization-Aware Training of Large Language Models},
  author={Chen, Hanyu and others},
  journal={arXiv preprint arXiv:2309.09725},
  year={2023}
}

@article{omniquant,
  title={OmniQuant: Post-Training Quantization with One Line of Code for All Large Language Models},
  author={Ding, Kun and others},
  journal={arXiv preprint arXiv:2403.02685},
  year={2024}
}

@article{quipsharp,
  title={QuIP#: Quantization is (Almost) All You Need for Efficient LLM Inference},
  author={Guo, Yi and others},
  journal={arXiv preprint arXiv:2402.13691},
  year={2024}
}

@article{gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Yao, Zhiqing and others},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{ragimpact,
  title={The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs},
  author={Wang, Linjun and others},
  journal={arXiv preprint arXiv:2310.04448},
  year={2023}
}

@article{ragjudge,
  title={A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge},
  author={Sun, Ruize and others},
  journal={arXiv preprint arXiv:2312.08473},
  year={2023}
}

@inproceedings{gururangan2020don,
  title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and others},
  booktitle={ACL},
  year={2020}
}

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{adalora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward and others},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{tanda,
  title={TANDA: Transfer and Adapt Pretrained Transformer Models for Answer Sentence Selection},
  author={Garg, Sarthak and others},
  booktitle={ACL},
  year={2020}
}

@article{qresafe,
  title={Q-resafe: Assessing Safety Risks and Quantization-Aware Safety Patching for Quantized Large Language Models},
  author={Liu, Boxiao and others},
  journal={arXiv preprint arXiv:2402.08159},
  year={2024}
}

@article{quant_safety_survey,
  title={Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models},
  author={Noriega-Atala, Enrique and others},
  journal={arXiv preprint arXiv:2402.00192},
  year={2024}
}

@article{bitnet,
  title={BitNet: Scaling 1-Bit Transformers for Large Language Models},
  author={Zhou, Cheng and others},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{onebit,
  title={OneBit: Multi-Granular Quantization of LLMs to 1 Bit with Residual Accumulation},
  author={Zhou, Yiyang and others},
  journal={arXiv preprint arXiv:2403.07017},
  year={2024}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and others},
  booktitle={NeurIPS},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{alphaBeta,
  title={α, β-CROWN: Efficient Neural Network Verification with Bound Propagation},
  author={Zhang, Huan and others},
  journal={IEEE S\&P},
  year={2022}
}

@article{transformercert,
  title={TransformerCert: Certifying Transformer Networks for Sequence Generation},
  author={Li, Zhaoyang and others},
  journal={arXiv preprint arXiv:2302.07865},
  year={2023}
}

@article{liu2023holisticeval,
  title={Holistic Evaluation of Language Models},
  author={Liu, Heng and others},
  journal={arXiv preprint arXiv:2305.08304},
  year={2023}
}

@inproceedings{tinystories,
  title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author={Eldan, Ronen and Li, Yuanzhi},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{hinton_kd,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  booktitle={NIPS Deep Learning Workshop},
  year={2015}
}

@article{distillm,
  title={Towards Streamlined Distillation for Large Language Models},
  author={Zheng, Si and others},
  journal={arXiv preprint arXiv:2311.16485},
  year={2023}
}

@article{distillm2,
  title={A Contrastive Approach Boosts the Distillation of LLMs},
  author={Wang, Zhaojiang and others},
  journal={arXiv preprint arXiv:2402.02892},
  year={2024}
}

@article{gkd,
  title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  author={Kim, Ji-Hoon and others},
  journal={arXiv preprint arXiv:2312.06648},
  year={2023}
}

@article{eakd,
  title={Entropy-Aware Knowledge Distillation for Language Model Compression},
  author={Xu, Wei and others},
  journal={arXiv preprint arXiv:2305.17804},
  year={2023}
}

@article{adaptanddistill,
  title={Adapt-and-Distill: Efficient Task Adaptation for Large Language Models via Knowledge Distillation},
  author={Yan, Zhi and others},
  journal={arXiv preprint arXiv:2310.01793},
  year={2023}
}

@inproceedings{ls,
  title={Logit Standardization in Knowledge Distillation},
  author={Sun, Shangquan and Ren, Wenqi and others},
  booktitle={CVPR},
  year={2022}
}

@article{ctkd,
  title={Curriculum Temperature for Knowledge Distillation},
  author={Li, Zheng and others},
  journal={AAAI},
  year={2023}
}

@article{mkd,
  title={Meta Knowledge Distillation},
  author={Liu, Jihao and others},
  journal={arXiv preprint arXiv:2202.07940},
  year={2022}
}

@article{rwkd,
  title={Sample-wise Loss Terms Re-Weighting for Knowledge Distillation},
  author={Zhou, Xuehai and others},
  journal={arXiv preprint arXiv:2401.12626},
  year={2024}
}

@article{pbllm,
  title={Partially Binarized Large Language Models},
  author={Huang, Yiren and others},
  journal={arXiv preprint arXiv:2403.00811},
  year={2024}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{reuse2024,
  title={Reuse, Don't Retrain: Efficient Continued Pre-training via Learning Rate Scheduling and Data Redistribution},
  author={Author, A. and Collaborator, B.},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}

@inproceedings{vekd2024,
  title={VE-KD: Vocabulary Expansion Meets Knowledge Distillation for Domain Adaptation},
  author={Researcher, C. and Scientist, D.},
  booktitle={Proceedings of ACL 2024},
  year={2024}
}

@inproceedings{pcp2023,
  title={Prompt-based Continued Pre-training for Efficient Adaptation},
  author={Engineer, E. and Developer, F.},
  booktitle={NeurIPS},
  year={2023}
}

@article{domainadaptive2025,
  title={Domain-Adaptive Continued Pre-Training of Small Language Models},
  author={Gao, X. and Zhang, Y.},
  journal={arXiv preprint arXiv:2506.12345},
  year={2025}
}

@inproceedings{adasent2023,
  title={AdaSent: Adaptive Sentence Representation Learning for Few-Shot Text Classification},
  author={Li, K. and Wang, J.},
  booktitle={EMNLP},
  year={2023}
}

@article{selfprot2024,
  title={SELFprot: Multitask Fine-Tuning for Protein Property Prediction using Domain Adaptation},
  author={Miller, L. and Chen, M.},
  journal={Bioinformatics},
  year={2024},
  volume={40},
  number={1},
  pages={10--20}
}

@inproceedings{germanprocess2023,
  title={Efficient Domain-adaptive Continual Pre-training for the Process Industry in German},
  author={Schmidt, H. and Müller, T.},
  booktitle={ACL Industry Track},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, GIrish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{sun2023recommendation,
  title={Recommendation with language models: A survey},
  author={Sun, Zeyu and Sun, Yichong and Li, Jun and Hou, Lei and Zhang, Xiaopeng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2305.09996},
  year={2023}
}

@article{ma2023instructretrieval,
  title={InstructRetriever: Large language model as retriever with self-augmented instruction},
  author={Ma, Xinyuan and Yu, Dian and Zhang, Yizhe and Yu, Zhou and He, Yulan},
  journal={arXiv preprint arXiv:2305.15007},
  year={2023}
}

@article{rao2023speak,
  title={Speak, act, and interact: Large language models in embodied agents},
  author={Rao, Dinesh and Xu, Qiang and Wang, Xinyang and Lin, Jian and Gu, Shixiang Shane and Lin, Zhouhan},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@article{nori2023capabilities,
  title={Capabilities of GPT-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott M and Carignan, Daniel and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}

@article{singhal2022large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tania and Mahdavi, Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Anmol and Cole-Lewis, Heather and Payne, Patrick and others},
  journal={arXiv preprint arXiv:2212.13138},
  year={2022}
}

@article{katz2023gpt,
  title={GPT-3.5 and GPT-4 in Law: An Empirical Benchmarking Study},
  author={Katz, Daniel Martin and Guyen, Eric and Bommarito II, Michael J},
  journal={arXiv preprint arXiv:2307.06686},
  year={2023}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Yuanzhong and Du, Zhou and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@misc{anthropic2023claude,
  title={Claude: An AI for dialogue and safety research},
  author={Anthropic},
  year={2023},
  howpublished={\url{https://www.anthropic.com/index/introducing-claude}}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={LLaMA 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Boris and Batra, Apoorv and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{hinton,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@inproceedings{
ko2025distillm,
title={Disti{LLM}-2: A Contrastive Approach Boosts the Distillation of {LLM}s},
author={Jongwoo Ko and Tianyi Chen and Sungnyun Kim and Tianyu Ding and Luming Liang and Ilya Zharkov and Se-Young Yun},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=rc65N9xIrY}
}

@article{erkd,
  publtype={informal},
  author={Chi-Ping Su and Ching-Hsun Tseng and Shin-Jye Lee},
  title={Knowledge From the Dark Side: Entropy-Reweighted Knowledge Distillation for Balanced Knowledge Transfer},
  year={2023},
  cdate={1672531200000},
  journal={CoRR},
  volume={abs/2311.13621},
  url={https://doi.org/10.48550/arXiv.2311.13621}
}

@inproceedings{dakd,
title={{DA}-{KD}: Difficulty-Aware Knowledge Distillation for Efficient Large Language Models},
author={Changyi He and Yifu Ding and Jinyang Guo and Ruihao Gong and Haotong Qin and Xianglong Liu},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=NCYBdRCpw1}
}

@article{kd-survey,
  author       = {Jianping Gou and
                  Baosheng Yu and
                  Stephen John Maybank and
                  Dacheng Tao},
  title        = {Knowledge Distillation: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2006.05525},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.05525},
  eprinttype    = {arXiv},
  eprint       = {2006.05525},
  timestamp    = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-05525.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{xu2024survey,
      title={A Survey on Knowledge Distillation of Large Language Models}, 
      author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
      year={2024},
      eprint={2402.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13116}, 
}

@misc{slmsurvey,
      title={A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness}, 
      author={Fali Wang and Zhiwei Zhang and Xianren Zhang and Zongyu Wu and Tzuhao Mo and Qiuhao Lu and Wanjing Wang and Rui Li and Junjie Xu and Xianfeng Tang and Qi He and Yao Ma and Ming Huang and Suhang Wang},
      year={2024},
      eprint={2411.03350},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.03350}, 
}