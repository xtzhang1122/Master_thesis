\section{Related Work}

This chapter provides the theoretical and conceptual background that grounds this research. 
Section \hyperref[subsec: ai_overview]{2.1} offers a concise overview of Artificial Intelligence, Machine Learning, Deep Learning, and the evolution of Large Language Models (LLMs), establishing the foundation for the subsequent discussions. 
Section \hyperref[subsec: kd]{2.2} introduces the fundamental principles of Knowledge Distillation (KD), including the teacher–student framework and its core objective of transferring knowledge from large to compact models. 
Section \hyperref[subsec: advance_kd]{2.3} examines recent advances and variants of KD with a particular emphasis on their application to LLMs. 
Section \hyperref[subsec: da]{2.4} explores the theoretical basis of Domain Adaptation (DA) and its role in addressing distributional shifts between training and target domains. 
Section \hyperref[subsec: int]{2.5} then discusses emerging efforts to integrate KD and DA, highlighting their complementary nature and the challenges inherent in unifying these paradigms for domain-robust distillation. 
Finally, Section \hyperref[subsec: claim]{2.6} synthesizes the limitations of existing methods and articulates the motivation for developing the proposed \textbf{Domain-Adaptive Entropy-Selective Knowledge Distillation (DA-ESKD) framework}.

We organize prior work into three broad directions: (i) reshaping the knowledge distillation (KD) objective, (ii) incorporating difficulty- or uncertainty-awareness into supervision, and (iii) adapting models to new domains through continued pretraining or vocabulary changes. We move from objective-level modifications, to approaches that explicitly reason about instance difficulty, and finally to domain-oriented strategies. Each thread provides context for how our proposed DA-ESKD departs from existing practices.

\subsection{Overview of Artificial Intelligence}
\label{subsec: ai_overview}

\subsection{Knowledge Distillation}
\label{subsec: kd}

\subsection{Advancements in Knowledge Distillation}
\label{subsec: advance_kd}

\subsection{Domain Adaptation}
\label{subsec: da}

\subsection{Integrating Domain Adaptation and Knowledge Distillation for Large Language Models}
\label{subsec: int}

\subsection{Poposed Approach}
\label{subsec: claim}



\subsection{Objective reshaping.}
A first line of work modifies the KD loss while still supervising broadly across data. 
\textbf{GKD} \cite{gkd} adopts an on-policy strategy: the student trains on its own generations and receives token-level feedback via a generalized JSD objective, directly reducing the train–inference mismatch. 
\textbf{DistiLLM} \cite{distillm} and its contrastive extension \textbf{DistiLLM2} \cite{ko2025distillm} instead refine off-policy supervision, using skewed KL divergence and contrastive penalties to sharpen the student distribution on teacher-dispreferred tokens. 
\textbf{Logit Standardization (LS)} \cite{ls} tackles instability from mismatched logit scales by applying Z-score normalization before temperature scaling, preserving geometric structure prior to softmax. 
These approaches improve optimization but typically still require teacher queries across nearly the entire training corpus, so compute cost grows with dataset size.

\subsection{Difficulty-aware supervision.}
A second set of methods introduces difficulty signals. 
Some approaches maintain full-corpus supervision but reweight examples: \textbf{EA-KD} \cite{eakd} and \textbf{RW-KD} \cite{rwkd} emphasize uncertain instances, while \textbf{ER-KD} \cite{erkd} directly scales per-sample loss by the teacher’s predictive entropy, steering the student toward high-entropy examples. 
Other approaches move beyond reweighting to actual data pruning. 
\textbf{DA-KD} \cite{dakd}, for instance, constructs a smaller but harder training set via a Distillation Difficulty Score (DDS) defined by teacher–student loss ratios, prunes easy samples, and reinjects a fraction of them for diversity. To stabilize training on these challenging subsets, it introduces a Bidirectional Discrepancy Loss (BDL) that bounds gradients and balances teacher–student divergence. Such difficulty-based methods reduce redundancy and improve efficiency, but still require extensive teacher scoring to estimate difficulty in the first place.

\subsection{Domain adaptation.}
Orthogonal to objective and difficulty-based strategies, another large body of work addresses domain shift. 
\textbf{Adapt-and-Distill} \cite{adaptanddistill} demonstrates that continual pretraining in-domain before KD improves accuracy, though at nearly double the compute cost. 
\textbf{AdaLM} expands the tokenizer to mitigate subword drift, reducing out-of-vocabulary issues but lengthening sequences. Continued pretraining more broadly has shown consistent gains: 
\textbf{Reuse, Don’t Retrain} \cite{reuse2024} optimizes schedules for large models with $\sim$9\% improvements; 
\textbf{Gururangan et al.} \cite{gururangan2020don} report robust cross-domain benefits across multiple domains; 
\textbf{PCP} \cite{pcp2023} leverages prompt templates during CPT; and \textbf{Domain-Adaptive CPT for small LMs} \cite{domainadaptive2025} develops more efficient CPT for low-resource settings. 
\textbf{VE-KD} \cite{vekd2024} goes further by coupling vocabulary expansion with KD, surpassing Adapt-and-Distill on biomedical tasks while reducing training time. 
Beyond these general-purpose methods, several works adapt KD pipelines to specialized domains—few-shot classification \cite{adasent2023}, protein multitask regression \cite{selfprot2024}, or non-English corpora \cite{germanprocess2023}. For example, augmenting KD with a $k$-NN pipeline on German industrial IR improves accuracy while cutting GPU use by $\sim$4$\times$. 
These domain-oriented strategies consistently improve transfer, but often at the expense of higher pretraining or tokenization costs.

\subsection{Our approach in context.}
\textbf{DA-ESKD} combines the strengths of selective supervision and lightweight adaptation. Rather than reweighting after full teacher computation (EA-KD/RW-KD/ER-KD) or broadly scoring before pruning (DA-KD), it gates teacher queries by the student’s own entropy, only consulting the teacher on uncertain cases. This avoids unnecessary teacher calls on easy examples. In parallel, a single-epoch masked-LM warm-up on in-domain text provides most of the domain benefit of continued pretraining without its full cost. Together, these design choices yield a favorable quality--cost trade-off by unifying compute-aware selective querying with minimal domain adaptation.
